# AI-Powered Cloud Cost Optimizer (LLM-Driven)

## ğŸ“Œ Project Overview

This project is an **AI-powered Cloud Cost Optimizer** that helps users understand and optimize cloud costs based on a **plain-English project description**.

Instead of manually estimating infrastructure costs, the system:

1. Extracts a structured project profile using an LLM  
2. Generates realistic, budget-aware synthetic cloud billing  
3. Analyzes costs against the budget  
4. Produces actionable, multi-cloud cost optimization recommendations  
5. Operates through a **menu-driven CLI**

The project demonstrates **backend development**, **LLM integration**, and **robust handling of probabilistic AI outputs**.

---

## ğŸ—ï¸ High-Level Architecture & Design

AI-Powered Cloud Cost Optimizer (LLM-Driven)
ğŸ“Œ Project Overview

This project is an AI-powered Cloud Cost Optimizer that helps users understand and optimize cloud costs based on a plain-English project description.

Instead of manually estimating infrastructure costs, the system:

Extracts a structured project profile using an LLM

Generates realistic, budget-aware synthetic cloud billing

Analyzes costs against the budget

Produces actionable, multi-cloud cost optimization recommendations

Operates through a menu-driven CLI

The project demonstrates backend development, LLM integration, and robust handling of probabilistic AI outputs.



---

## ğŸ§  Key Design Decisions

### 1ï¸âƒ£ Separation of Concerns

Each responsibility is isolated into its own module:

- **Profile Extraction** â†’ LLM-based
- **Billing Generation** â†’ LLM-based
- **Cost Analysis** â†’ Pure Python (deterministic)
- **Recommendations** â†’ LLM-based
- **CLI** â†’ Orchestration only (no business logic)

This ensures the system is **modular, testable, and production-oriented**.

---

### 2ï¸âƒ£ Dual LLM Call Strategy (Important Design Choice)

Different LLM decoding strategies behave differently.  
To ensure reliability, **two LLM call modes** are used intentionally.

| Task | LLM Call Type | Reason |
|----|----|----|
| Project Profile Extraction | Streaming (`chat_stream`) | Small output, conservative JSON |
| Synthetic Billing | Non-streaming (`chat`) | Large structured JSON arrays |
| Recommendations | Non-streaming (`chat`) | Reasoning-heavy, long output |

This avoids JSON corruption and empty outputs â€” a **real-world LLM engineering concern**.

---

### 3ï¸âƒ£ Defensive Validation of LLM Outputs

Because LLMs are probabilistic:

- All LLM outputs are validated
- Empty or malformed JSON is rejected early
- Corrupted artifacts are never propagated to downstream stages

This guarantees **pipeline stability**.

---

### 4ï¸âƒ£ Absolute Paths for Artifacts

All file paths are resolved from the project root to ensure consistent behavior across:

- CLI execution
- Test scripts
- Windows environments

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ project_description.txt
â”‚   â”œâ”€â”€ project_profile.json
â”‚   â”œâ”€â”€ mock_billing.json
â”‚   â””â”€â”€ cost_optimization_report.json
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ profile_prompt.yaml
â”‚   â”œâ”€â”€ billing_prompt.yaml
â”‚   â”œâ”€â”€ recommendation_prompt.yaml
â”‚   â””â”€â”€ summary_prompt.yaml
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ profile_extractor.py
â”‚   â”‚   â”œâ”€â”€ billing_generator.py
â”‚   â”‚   â”œâ”€â”€ cost_analyzer.py
â”‚   â”‚   â””â”€â”€ recommendation_engine.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ profile_pipeline.py
â”‚   â”‚   â”œâ”€â”€ billing_pipeline.py
â”‚   â”‚   â”œâ”€â”€ cost_analysis_pipeline.py
â”‚   â”‚   â””â”€â”€ recommendation_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ llm_client.py
â”‚   â”‚
â”‚   â”œâ”€â”€ constant/
â”‚   â”‚   â””â”€â”€ paths.py
â”‚   â”‚
â”‚   â””â”€â”€ exception.py
â”‚
â”œâ”€â”€ cost_optimizer.py        # CLI entry point
â”œâ”€â”€ test_profile_pipeline.py
â”œâ”€â”€ test_billing_pipeline.py
â”œâ”€â”€ test_cost_analysis_pipeline.py
â”œâ”€â”€ test_recommendation_pipeline.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env_example
â””â”€â”€ README.md



âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone the Repository
git clone <your-repository-url>
cd OpenText_project

2ï¸âƒ£ Create and Activate Virtual Environment (Recommended: Conda)
conda create -n cloud-opt python=3.10 -y
conda activate cloud-opt


3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Configure Environment Variables

Create a .env file using .env_example as reference:

MISTRAL_API_KEY=your_mistral_api_key_here


â–¶ï¸ How to Run the Project
Start the CLI
python cost_optimizer.py

CLI Menu Options
1. Enter new project description
2. Run Complete Cost Analysis
3. View Recommendations
4. Export Report
5. Exit

Example Workflow

1ï¸âƒ£ Enter a project description in plain English
2ï¸âƒ£ Run complete cost analysis
3ï¸âƒ£ View optimization recommendations
4ï¸âƒ£ Export the final report

All generated artifacts are saved in the artifacts/ directory.



| File                          | Description                      |
| ----------------------------- | -------------------------------- |
| project_description.txt       | Raw user input                   |
| project_profile.json          | Structured project profile       |
| mock_billing.json             | Synthetic cloud billing          |
| cost_optimization_report.json | Final analysis & recommendations |


# LLM Usage & Academic Integrity

LLMs are used only where reasoning or generation is required

Deterministic logic (cost calculations) is handled in Python

Different decoding strategies are used intentionally

All LLM outputs are validated

# Tools Used

Mistral API â€“ LLM inference - Not used Hugging face as mistral was not working but when i used mistral api from it's officaial website it was working fine

Python 3.10

dotenv

PyYAML

ChatGPT


